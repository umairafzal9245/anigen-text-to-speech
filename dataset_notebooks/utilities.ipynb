{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if all audio files are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalfilesindir = os.listdir('wavs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelsheet = open('extracteddata.csv',encoding='utf-8',mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalfilenames = []\n",
    "for record in excelsheet:\n",
    "    tok = record.split(',')\n",
    "    filename = tok[3]\n",
    "    totalfilenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in totalfilenames:\n",
    "    if filename not in totalfilesindir:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of speaker details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelsheet = open('extracteddata.csv',encoding='utf-8',mode='r')\n",
    "\n",
    "emails = []\n",
    "for record in excelsheet:\n",
    "    tok = record.split(',')\n",
    "    email = tok[2]\n",
    "    gender = tok[1]\n",
    "    if (email,gender) not in emails:\n",
    "        emails.append((email,gender))\n",
    "\n",
    "emailswithid = {}\n",
    "i = 0\n",
    "for item in emails:\n",
    "    emailswithid[item] = i \n",
    "    i += 1\n",
    "\n",
    "ff3 = open('speakerdetails.csv',encoding='utf-8',mode='w')\n",
    "for item in emailswithid:\n",
    "    ff3.write(str(item[0])+','+str(emailswithid[item])+','+str(item[1])+'\\n')\n",
    "\n",
    "    \n",
    "ff3.close()\n",
    "excelsheet.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structing data as VCTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelsheet = open('extracteddata.csv',encoding='utf-8',mode='r')\n",
    "\n",
    "def findnth(haystack, needle, n):\n",
    "    parts= haystack.split(needle, n+1)\n",
    "    if len(parts)<=n+1:\n",
    "        return -1\n",
    "    return len(haystack)-len(parts[-1])-len(needle)\n",
    "\n",
    "items = []\n",
    "for item in excelsheet:\n",
    "    tok = item.split(',')\n",
    "    location = findnth(item,',',5)\n",
    "    emailwithgender = (tok[2],tok[1])\n",
    "    nam = tok[3]\n",
    "    sentence = item[location+1:]\n",
    "    idd = emailswithid[emailwithgender]\n",
    "    items.append('wavs/'+nam+'|'+str(idd)+'|'+sentence)\n",
    "    \n",
    "excelsheet.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing sentences with english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "\n",
    "englishcleaned = []\n",
    "for item in items:\n",
    "    tokens = item.split('|')\n",
    "    sent = tokens[2]\n",
    "    result = re.search('[a-zA-Z]\\S+',sent)\n",
    "    if result == None:\n",
    "        englishcleaned.append(item)\n",
    "    else:\n",
    "        if os.path.isfile(tokens[0]):\n",
    "            os.remove(tokens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing english numbers with urdu numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(englishcleaned)):\n",
    "    tok = englishcleaned[i].split('|')\n",
    "    tok[2] = tok[2].replace('0','۰').replace('1','۱').replace('2','۲').replace('3','۳').replace('4','۴').replace('5','۵').replace('6','۶').replace('7','۷').replace('8','۸').replace('9','۹')\n",
    "    englishcleaned[i] = '|'.join(tok)\n",
    "\n",
    "\n",
    "ff2 = open('urdumetadatauncleaned.csv',encoding='utf-8',mode='w')\n",
    "for item in englishcleaned:\n",
    "    ff2.write(item)\n",
    "ff2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "filelist = os.listdir('wavs')\n",
    "excelsheet = open('urdumetadatauncleaned.csv',encoding='utf-8',mode='r')\n",
    "totalfilenames = []\n",
    "for record in excelsheet:\n",
    "    tok = record.split('|')\n",
    "    filename = tok[0][5:]\n",
    "    totalfilenames.append(filename)\n",
    "\n",
    "for filename in totalfilenames:\n",
    "    if filename not in filelist:\n",
    "        print(filename)\n",
    "\n",
    "for filename in filelist:\n",
    "    if filename not in totalfilenames:\n",
    "        print(filename)\n",
    "\n",
    "excelsheet.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('urdumetadatacleaned.csv',mode='r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaldata = []\n",
    "for item in file:\n",
    "    data = []\n",
    "    data.append(item.replace('\\n',''))\n",
    "    totaldata.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(totaldata,test_size=0.02)\n",
    "\n",
    "traindata,val = train_test_split(train,test_size=0.01)\n",
    "\n",
    "tf = open('trainfile.txt',mode='w',encoding='utf-8')\n",
    "vf = open('valfile.txt',mode='w',encoding='utf-8')\n",
    "ttf = open('testfile.txt',mode='w',encoding='utf-8')\n",
    "\n",
    "for item in traindata:\n",
    "    tf.write(item[0]+'.\\n')\n",
    "\n",
    "for item in test: \n",
    "    ttf.write(item[0]+'.\\n')\n",
    "\n",
    "for item in val:\n",
    "    vf.write(item[0]+'.\\n')\n",
    "\n",
    "tf.close()\n",
    "vf.close()\n",
    "ttf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذ\n",
      "ک\n",
      "ش\n",
      "ا\n",
      "ب\n",
      "ں\n",
      "ح\n",
      "ء\n",
      "ٹ\n",
      "گ\n",
      "ظ\n",
      "ی\n",
      "د\n",
      "س\n",
      "ر\n",
      "ہ\n",
      "م\n",
      "ت\n",
      "ن\n",
      "و\n",
      "ط\n",
      "ع\n",
      "ف\n",
      " \n",
      "ل\n",
      "ے\n",
      "چ\n",
      "ج\n",
      "،\n",
      "پ\n",
      "ھ\n",
      "خ\n",
      "آ\n",
      "ق\n",
      "ئ\n",
      "غ\n",
      "ص\n",
      "ز\n",
      "ث\n",
      "ِ\n",
      "‘\n",
      "’\n",
      "ڈ\n",
      "ڑ\n",
      "ض\n",
      "۱\n",
      "۰\n",
      "ؤ\n",
      "ُ\n",
      "۲\n",
      "۷\n",
      "۵\n",
      "۶\n",
      "ٰ\n",
      "۴\n",
      "'\n",
      "ّ\n",
      "ً\n",
      "؟\n",
      "ٔ\n",
      "۳\n",
      "َ\n",
      "ﷺ\n",
      "ﷲ\n",
      "۸\n",
      "۹\n",
      "ۓ\n",
      "/\n",
      "-\n",
      "ؐ\n",
      "٪\n",
      "!\n",
      "؛\n",
      "ؒ\n",
      "ژ\n",
      ":\n",
      "ۃ\n",
      "ؑ\n",
      "ۖ\n",
      ".\n",
      "ۂ\n",
      "أ\n",
      "ﺅ\n",
      "ؓ\n",
      ",\n",
      "‌\n"
     ]
    }
   ],
   "source": [
    "file = open('urdumetadatacleaned.csv',encoding='utf-8',mode='r')\n",
    "symbols = []\n",
    "for item in file:\n",
    "    tok = item.split('|')\n",
    "    phon = tok[2].replace('\\n','')\n",
    "    phonlis = list(phon)\n",
    "    uniq = set(phonlis)\n",
    "    for it in uniq:\n",
    "        symbols.append(it)\n",
    "        \n",
    "file.close()\n",
    "uniqsymbols = []\n",
    "for it in symbols:\n",
    "    if it not in uniqsymbols:\n",
    "        uniqsymbols.append(it)\n",
    "\n",
    "for i in uniqsymbols:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import FrozenSet\n",
    "\n",
    "_pad        = '_'\n",
    "\n",
    "URDU_ALPHABETS: FrozenSet[str] = frozenset(\"آ أ ا ب پ ت ٹ ث ج چ ح خ د ڈ ذ ر ڑ ز ژ س ش ص ض ط ظ ع غ ف ق ک گ ل م ن ں و ؤ ﺅ ہ ۂ ۃ ھ ء ی ئ ے ۓ\".split())\n",
    "\n",
    "URDU_DIGITS: FrozenSet[str] = frozenset(\"۰ ۱ ۲ ۳ ۴ ۵ ۶ ۷ ۸ ۹\".split())\n",
    "\n",
    "URDU_PUNCTUATIONS: FrozenSet[str] = frozenset(\"؟ ٪ ! ، ’ ‘ ' . , - ؛ :\".split())\n",
    "\n",
    "URDU_DIACRITICS: FrozenSet[str] = frozenset(\"\\u064e \\u064B \\u0670 \\u0650 \\u064F \\u064d \\u0651 \\u0654\".split())\n",
    "\n",
    "URDU_EXTRA_CHARACTERS: FrozenSet[str] = frozenset(\"ﷲ ﷺ  ؓ  ؑ  ؒ  ؐ  ۖ\".split())\n",
    "\n",
    "URDU_ALL_CHARACTERS: FrozenSet[str] = frozenset().union(URDU_ALPHABETS, URDU_DIGITS, URDU_PUNCTUATIONS,  # type: ignore\n",
    "                                                        URDU_DIACRITICS, URDU_EXTRA_CHARACTERS)  # type: ignore\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(URDU_ALL_CHARACTERS) \n",
    "\n",
    "\n",
    "print(len(symbols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking audio details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "sampling_rate, data = read('testaudio.aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 329, 303, 277], dtype=int16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(data, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
